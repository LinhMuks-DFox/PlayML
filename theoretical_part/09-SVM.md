# 支撑向量机

目录：

* [什么是支撑向量机？](#what-is-SVM)
* [SVM背后的最优化问题](#The-Optimisation-Problem-Behind-SVM)



#### <span id="what-is-SVM">支撑向量机</span>

支撑向量机，简写为SVM，是其Supported Vector Machine的缩写。使用SVM可以解决回归问题以及分类问题。（*本笔记中主要尝试采用SVM来完成分类问题，在末尾会撰写如何使用SVM解决回归问题的相关内容。*）

回顾逻辑回归算法，通过运算，逻辑回归算法获取到了一个决策边界，在边界的一侧就是1，另一侧就是0等。

但是逻辑回归算法中，决策边界不是唯一的：

<p style="align:center"><img src="./pngs/SVM_1.png" style="zoom:60%; "/></p>

对于红蓝的分类来说，A于B都是一个可以正确分类的决策边界，对于这种决策边界不唯一的问题，通常有一个专门的术语：**不适定问题(ill-posed problem)**，逻辑回归在解决这种不适定问题的时候采用了这样的策略：

1. 定义一个概率函数Sigmoid
2. 根据这个概率函数进行建模，形成了一个损失函数
3. 最小化这个损失函数，求出决策边界

其中，损失函数是完全由训练数据集决定的。

SVM的策略稍微有些不同：

 来看一个例子：

<p style="align:center"><img src="./pngs/SVM_2.png" style="zoom:60%; "/></p>

现在通过算法计算出了一个决策边界，获得了具体的模型，尝试实用其对为止的数据进行分类：

<p style="align:center"><img src="./pngs/SVM_3.png" style="zoom:60%; "/></p>

根据这个决策边界，这些绿色的点，显然会被分类为蓝色，实际上这些点距离红色更近，也就是说，其更大概率应该是红色，而不是蓝色。决策边界距离红色很近，所以很多接近红色的样本被误分类成了红色之外。所以这种情况下，模型的泛化能力可能比较差。

根据这种思想，什么样的决策边界泛化能力更好呢？Like this：

<p style="align:center"><img src="./pngs/SVM_4.png" style="zoom:60%; "/></p>

感性上来说这个直线作为决策边界更好，理论上这个直线有这样特征：

<p style="align:center"><img src="./pngs/SVM_5.png" style="zoom:60%; "/></p>

距离这跟直线最近的点（图中的2红1蓝），与这跟直线的距离，都尽可能地远。换句话说：我们希望决策边界距离红色的点远的同时，与蓝色的点距离也远，同时还能很好的分别红蓝数据点。

SVM的思想，并没有把模型的未来的泛化能力的考量寄望在数据的预处理阶段，或者是正则化阶段，而是放置在算法内部。

实际上这样的假设不仅仅是直观的假设，背后也是后数学理论的。数学上是可以证明在不适定问题上使用SVM找到的决策边界的泛化能力是好的。也正因如此，SVM也是统计学中，非常重要的方法，其背后是有很多统计理论的支撑。

对于SVM要找的决策边界：

1. 可以正确的分类
2. 距离两个类别的数据点尽可能地远(距离决策边界最近的数据点到决策边界的距离都尽可能的远)

基于这个考量，在这个红蓝分类案例中，这些特殊的点（距离决策边界最近的点），它们又定义出了两根直线：

<p style="align:center"><img src="./pngs/SVM_6.png" style="zoom:60%; "/></p>

这两根直线与SVM得到的决策边界相互平行，这两根直线定义的区域中，不再有任何的数据点。SVM最终得到的决策边界，相当于是这跟区域中间的那根线。

**总而言之，SVM尝试寻找一个最优的决策边界，距离两个类别的最近的样本最远。**

对于这些**最近的点**，就被称作**支撑向量(Supported Vector)**，本案中就是：

<p style="align:center"><img src="./pngs/SVM_7.png" style="zoom:60%; "/></p>

这样的三个点。

支撑向量定义了一个区域，这跟区域定义了决策边界，决策边界就是这跟区域的中间的那根线。换句话说，最终找到的最优的决策边界是被这些支撑向量定义的。正因如此，支撑向量是这跟算法中最重要的元素。

在这个案例中，这三根直线（A,B,C）之间有一个距离$d$，本案例中上下两根直线到决策边界的距离都是一样的，同为$d$：

<p style="align:center"><img src="./pngs/SVM_8.png" style="zoom:60%; "/></p>



确定这跟区域两根直线A与B之间的距离，通常叫做**margin**，其大小就是$2 \times d$

SVM需要最大化这跟**margin**。这跟例子是一个**线性可分问题**，简单来说就是假设这些样本点之间存在一根可以划分其的直线/在高维空间中存在一个超平面可以划分这些样本点，这种情况下才定义出了margin。

这种算法一般叫做**Hard Margin SVM**，算法是非常严格的，切切实实地找到了决策边界，完全正确地划分了样本点同时最大化了margin。

真实情况下，许多数据是线性不可分地，SVM可以进一步改进，得到**Soft Margin SVM**算法。Soft Margin SVM是基于Hard Margin SVM改进出来的算法，所以后续部分会先介绍Hard Margin SVM。

最后，如何使用数学语言来描述“最大化Margin”呢？

#### <span id="The-Optimisation-Problem-Behind-SVM">SVM背后的最优化问题</span>
