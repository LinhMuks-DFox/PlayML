# 多项式回归和模型泛化

对于线性回归算法来说，要求预测的结果与输入数据之间存在明显的线性关系，但是更多的时候，其关系可能是非线性的，本章将引入一个新的算法，来解决这个问题。

### 目录

* [多项式回归](#Polynomial-Regression)
* [过拟合和欠拟合](#Overfit-Underfit)

### <span id="Polynomial-Regression">多项式回归</span>

==本节的代码比较多，代码里也有许多应用的说明，结合代码一起食用==

对于含有线性关系的数据，只需要找出拟合输入与输出的直线（精确一点应该是用于确定这根直线的系数），就可以解决问题。

如果一组数据的输入与输出之前，存在2次曲线的关系，那么我们所需要求解的曲线的表达式就是
$$
y = ax^2 + bx +c
$$
这个式子虽然叫做二次方程，但是如果：

* 将$x^2$强行解释成一个特征
* 将$x$解释成一个特征

本来特征只有一个，标记也只有一个，经过这样的解释，数据集就有了两个特征+一个标记。这样来理解的话，这样的一个式子依然是一个**线性的**，但是，从$x$的角度来看，这就是**非线性的**了。

给原有的样本添加了一些新的特征，这些特征是原来样本的多项式项，比如$x^2$，就是对$x$进行平方，增加了这些特征之后，就可以使用线性回归的思路更好的拟合原来的数据，但是究其本质，是对原来的特征而言，这种非线性的曲线。

 参考[notebook](#../notebooks/chp6-Polynomial-Regression-and-Model-Generalization/01-Polynomial-Regression.ipynb)

通过代码演示，可以看到多项式回归并没有什么新的机器学习的思路，只是添加了更多次数的项，作为新的特征。这个和PCA算法对数据进行降为的操作是相反的。

### <span id="Overfit-Underfit">过拟合与欠拟合</span>

多项式回归固然好用，但是过度的使用，就很容易造成过拟合，使用不当也会造成欠拟合。

==这里将使用一个实际的例子来展示什么叫做过拟合和欠拟合，参考本节代码==

