# 多项式回归和模型泛化

对于线性回归算法来说，要求预测的结果与输入数据之间存在明显的线性关系，但是更多的时候，其关系可能是非线性的，本章将引入一个新的算法，来解决这个问题。

### 目录

* [多项式回归](#Polynomial-Regression)
* [过拟合和欠拟合](#Overfit-Underfit)
* [学习曲线](#Learning-Curve)
* [验证数据集与交叉验证](#validation-and-cross-validation)
* [偏差方差权衡](#Bias Variance Trade off)
* [模型的正则化](#Regularization)

### <span id="Polynomial-Regression">多项式回归</span>

==本节的代码比较多，代码里也有许多应用的说明，结合代码一起食用==

对于含有线性关系的数据，只需要找出拟合输入与输出的直线（精确一点应该是用于确定这根直线的系数），就可以解决问题。

如果一组数据的输入与输出之前，存在2次曲线的关系，那么我们所需要求解的曲线的表达式就是
$$
y = ax^2 + bx +c
$$
这个式子虽然叫做二次方程，但是如果：

* 将$x^2$强行解释成一个特征
* 将$x$解释成一个特征

本来特征只有一个，标记也只有一个，经过这样的解释，数据集就有了两个特征+一个标记。这样来理解的话，这样的一个式子依然是一个**线性的**，但是，从$x$的角度来看，这就是**非线性的**了。

给原有的样本添加了一些新的特征，这些特征是原来样本的多项式项，比如$x^2$，就是对$x$进行平方，增加了这些特征之后，就可以使用线性回归的思路更好的拟合原来的数据，但是究其本质，是对原来的特征而言，这种非线性的曲线。

参考[代码](../notebooks/chp6-Polynomial-Regression-and-Model-Generalization/01-Polynomial-Regression.ipynb)

补充，[Pipeline的知识点](../notebooks/chp6-Polynomial-Regression-and-Model-Generalization/02-Polynomial-Regression-and-Pipeline.ipynb)

通过代码演示，可以看到多项式回归并没有什么新的机器学习的思路，只是添加了更多次数的项，作为新的特征。这个和PCA算法对数据进行降为的操作是相反的。

### <span id="Overfit-Underfit">过拟合与欠拟合</span>

多项式回归固然好用，但是过度的使用，就很容易造成过拟合，使用不当也会造成欠拟合。

==这里将使用一个实际的例子来展示什么叫做过拟合和欠拟合，参考本节[代码](../notebooks/chp6-Polynomial-Regression-and-Model-Generalization/03-Overfit-and-Underfit.ipynb)==

通过例子可以看出，degree的数值越大，拟合的就越好，道理很简单，这么多样本点，总能找到一根曲线，这根曲线可以把所有的样本点都串起来，使得整体的MSE最小，甚至为0。

从degree=2到10到100，MSE一直在降低。

虽然从MSE的角度来看越来越小，但是，degree越高的曲线，越能反应样本走势吗？

答案是否定的，用一个非常高维的样本，产生了一根曲线，MSE确实很小，但是得到的曲线并不是想要的样子，别忘了最开始生产数据的时候，只是做了个2次曲线分布的样本。**为了拟合所有的样本点，曲线变得太过复杂了**，这就是所谓的**过拟合（Overfitting）**

例子最开始用的直线也没有很好的拟合数据样本，他的错误在于degree太低。不是太复杂，而是太简单，这就叫做**欠拟合（Underfitting）**

应用中，最常见的其实是过拟合。工作中主要解决的问题也就是这个问题。

比如例子中的过拟合的曲线：

<p style="align:center"><img src="./pngs/Polynomial-Regression-and-Model-Generalization_1.png" style="zoom:25%; "/></p>

红色的曲线是模型，蓝色的样本数据，使用这个过拟合的模型预测出一个结果，作为紫色的点。显然，紫色点并不在样本的分布趋势上。

这个预测值很有可能是错误的。换言之：模型拟合样本做的很好，但是对于新的样本来说，精确度就不足了。也可以说，**这个模型的泛化能力很弱，或者说由此及彼的能力很差**。训练模型的目的不在于拟合了多少原先的样本，在于拟合后，对于新来的数据能有多好的表现。

需要的是有泛化能力的模型。训练数据集和测试数据集的分离的做法，就很好的帮助我们发现模型是否有过拟合现象。

[回到代码](../notebooks/chp6-Polynomial-Regression-and-Model-Generalization/04-Why-Train-Test-Split.ipynb)

可以看到，通过这种方式，MSE的差别是越来越大的。有效的帮助我们发现了Overfit的情况。

代码中所实验的，是模型的复杂度，degree越高，模型越复杂。

通常来说，机器学习的算法都存在这样关系：

<p style="align:center"><img src="./pngs/Polynomial-Regression-and-Model-Generalization_2.png" style="zoom:25%; "/></p>

从欠拟合到合适到过拟合，不同的算法所获取的这个图可能是不一样的，但是整体的趋势是这样的。

总结来说

* 欠拟合，算法所训练的模型不能完整的表述数据关系，寻找到的特征可能太朴素。
* 过拟合，算法所训练的模型过多的表达了数据间的噪音关系，学习到的特征却又太细节了。
* 通过训练，测试分离的方法，可以帮助发现过拟合。

我们需要找到就是泛化能力最好的那里。

网格搜索的策略，使用不同参数的组合训练出模型，然后找到泛化能力最强的那个，也是这么个道理。

### <span id="Learning-Curve">学习曲线</span>

<p style="align:center"><img src="./pngs/Polynomial-Regression-and-Model-Generalization_2.png" style="zoom:25%; "/></p>

这张图描述的是模型复杂度和模型准确率的关系，其中可以很好的观察到过拟合，欠拟合等状况，同时这也是一个比较理论的图像，比较清晰，大多数情况下是没办法针对某一个模型画出这样清晰的曲线的。

并且有些模型并不适合绘制这样的图像，比如kNN算法，虽然内在是有这样的逻辑关系。

对于欠拟合，过拟合，还有另一个曲线可以描述：学习曲线。

**学习曲线：随着训练样本的逐渐增多，算法训练出的模型的表现能力。**

很好理解，人类学习知识的时候也是不断的将数据输入大脑，然后拟合各项数据，随着数据增多，对知识的掌握程度也就越好，对于机器学习也是一样的。

[参考说明代码](../notebooks/chp6-Polynomial-Regression-and-Model-Generalization/05-Learning-Curve.ipynb)

<table><tr>
 <td><img src="./pngs/Polynomial-Regression-and-Model-Generalization_4.png" style="zoom:50%; " border=0/></td>
<td><img src="./pngs/Polynomial-Regression-and-Model-Generalization_3.png" style="zoom:50%; " border=0/></td>
</tr></table>

对于过拟合：

* 训练数据集中，误差不大，甚至和最佳的情况差不多，更极端一些，degree更高的时候，训练数据集上的误差会更小
* 测试数据集中，距离训练数据集的误差比较远，这就意味着模型的泛化能力不行，对于新的数据来说误差比较大

<table><tr>
<td><img src="./pngs/Polynomial-Regression-and-Model-Generalization_4.png" style="zoom:50%; " border=0/></td>
<td><img src="./pngs/Polynomial-Regression-and-Model-Generalization_5.png" style="zoom:50%; " border=0/></td>
</tr></table>

对于欠拟合：

* 左右对比：趋于稳定的位置不同，欠拟合的情况相对最好的情况来说，稳定的位置更高
* 在欠拟合中，模型在训练数据集，测试数据集中都表现的不好，误差都比较大



### <span id="validation-and-cross-validation">验证数据集与交叉验证</span>

```mermaid
pie 
    title Data Split
    "训练数据集" : 75.00
    "测试数据集" : 25.00
```



测试数据集的意义：

1. 如果把所有的数据用作训练数据生产模型，容易**发生过拟合却不自知**
2. 分割成训练，测试数据集，通过测试数据集判断模型的好坏，帮助发现欠拟合/过拟合的情况。
3. 发生过拟合，欠拟合以后更换超参数，重新训练模型。



这就完全靠谱吗？当然，比只用训练数据来测试模型要靠谱的多。但是**他也有不靠谱的地方。**

比如：模型是否有可能针对特定测试数据集过拟合？

有没有可能，某个超参数下训练出来的模型，是针对测试数据集表现的优秀的？

因为发现模型测试数据集上表现不良的时候，会人为的调整参数，这有点“围绕着测试数据集打转”的意思，也就是“想办法找到一组参数，使得训练出来的模型在测试数据集上表现良好。”。

因为测试数据集是已知的，这种行为相当于，“针对测试数据集进行调参。”

如何解决呢？

很简单：把数据集分成三部分：

```mermaid
pie 
    title Data splits
    "训练数据集" : 60.00
    "验证数据集" : 20.00
    "测试数据集" : 20.00
```



操作步骤也很简单：

1. 使用训练数据集创建模型。
2. 使用验证数据集测试，针对验证数据集进行参数调整，直到找到一组参数，使得模型在验证数据集上表现最优。
3. 使用测试数据集进行测试，使用这个结果，作为最终衡量模型性能的数据集。

这样做，测试数据集是不参与模型的创建的。验证，训练数据集都参与了模型的创建。一个用来评判，一个用来训练，这两种操作都意味着参与了模型的创建。

测试数据集不参与创建模型，对于模型来说，测试数据是完全不可知的。相当于模拟了真实的环境。

这就意味着：

* 训练数据集，用于模型创建。
* 验证数据集，用于超参数调整。
* 测试数据集，最终衡量模型性能。

这个做法依然是存在问题的。

每一次的验证的数据，都是随机的从训练数据集中切出来的，训练的模型可能过拟合验证数据集，一旦验证数据集有比较极端的数据，可能导致模型不准确。

为了解决这个问题，就有了**交叉验证(Cross Validation)**



```mermaid
pie 
    title 对于原始数据的分割
    "训练数据集" : 75.00
    "测试数据集" : 25.00
```



然后将训练数据集分成$k$份（下图将k取3）：

```mermaid
pie 
    title 对于训练数据的分割
    "A" : 33.00
    "B" : 33.00
    "C" : 33.00
```

然后进行排列组合：

* 使用BC训练，用A做验证，产生模型$M_1$
* 使用AC训练，用B做验证，产生模型$M_2$
* 使用AB训练，用C做验证，产生模型$M_3$

这样就得到了三个模型，然后衡量这k个模型的性能，取平均，这平均值，就是调参的结果。

这样做，有一个求平均的过程，所以不会因为某一份验证数据集中有极端数据而产生偏差。

这样做比只设立一个验证数据集更加保险。

[参见代码](../notebooks/chp6-Polynomial-Regression-and-Model-Generalization/06-Validation-and-Cross-Validasion.ipynb)

把训练数据集分成k份训练，这种做法叫做**k-folds 交叉验证（k-folds cross validasion）**

缺点：每次都得训练k个模型，相当于整体性能满了k倍

极端情况下，k-folds 交叉验证可以变成**留一法LOO-CV（Leave-One-Out Cross Validation）**

训练数据集有m个样本，就把训练数据集分成m份，每次都将m-1份用于训练，用1个去验证，然后平均，作为准确度，这样做，完全不受随机的影响，最接近模型真正的性能指标。但是计算量奇大无比。学术中可能用的比较多，论文需要严谨嘛毕竟。



### <span id="Bias Variance Trade off">偏差方差权衡</span>

<p style="align:center"><img src="./pngs/Polynomial-Regression-and-Model-Generalization_6.png" style="zoom:25%; "/></p>

偏差（Bias），观察左下角的图，目标是中心的红点，打靶子的话，所有的子弹都偏离了中心的红点，这就是有比较高的偏差。

方差（Variance），观察右上角的图，基本都围绕着目标，但是子弹分散在目标的周围，描述这个分散的程度的值，就是方差。

对于机器学习来说，某个实际的问题就是目标，拟合的模型就是我们打出去的子弹，这就有可能犯偏差/方差的错误。

模型误差，来自于三方面：

* 偏差（Bias），和模型息息相关
* 方差（Variance），和模型息息相关
* 不可避免的误差（这部分错误是客观存在的，无能为力的，比如采集的数据本身就有噪音）

导致偏差（Bias）的主要原因：

* 对问题的假设不正确（比如对非线性数据使用线性回归）。
* 模型的Underfitting就是一个实际的会产生偏差的例子。
* 特征与想解决的问题没有关系，比如名字与成绩。

导致方差（Variance）的主要原因：

* 模型过于复杂，比如使用高阶多项式，数据一点点的扰动都会较大的影响模型，模型没有学习到实质的知识，而过于关注数据的噪音，学习到了过多的噪音。
* 模型Overfitting就是一个实际会产生方差的例子。

对于方差和偏差来说：

* 有些算法是天生的高方差（Hight Variance）算法，比如kNN。
* 非参数学习（kNN，决策树就是非参数学习）通常都是高方差算法，**因为不对数据进行任何假设**。
* 有一些算法是天生的高偏差（Hight Bias）算法， 比如线性回归。
* 参数学习通常都是高偏差算法，**因为对数据具有极强的假设**。

大多数算法具有相应的参数，可以调整偏差和方差，比如kNN中的k，线性回中使用多项式回归（调整degree）等。

偏差和方差通常是互相矛盾，互相制约的：

* 降低偏差，会提高方差
* 降低方差，会提高偏差

一个理想的机器学习模型，是低偏差，低方差的，实际情况下没这么理想，算法会出现错误，此时就要研究这个算法的错误主要是方差还是偏差，想办法使其平衡，不要太高的方差（模型泛化能力太差），不要太高的偏差（不然就偏离了问题的本质），错误本身不能避免，只能尽可能平衡。

调参实际就会在解决这个问题。**在机器学习领域，主要挑战来自于方差。（只局限于算法层面，不考虑问题，因为人类有时候对问题的理解太过于肤浅，比如对金融市场的理解，对疾病的理解）**

一直到我自己撰写这篇笔记为止，有不少人尝试使用历史的金融数据来预测未来的金融市场的走向，但是结果都是不尽理想的。因为愚蠢的人类总是做一些愚蠢的假设，使用的数据和问题之间没什么联系，这就引进了巨大的偏差。

如果有一个神仙，可以使得选择的数据和遇到的问题之间有良好的联系，那么，进一步的主要的挑战，也许就是不偏差，而是方差了。

在实际的机器学习生产领域，遇到方差过大的问题，一般有以下几个解决方法：

* 降低模型的复杂度（比如多项式回归中的Degree，比如kNN的k）。

* 减少数据维度，降噪。

* 增加样本数量（模型太复杂，可是样本数量太少，使得样本数量不足以支撑计算出模型中复杂的参数，比如神经网络，深度学习）。

* 使用验证集。

* <p style="color:red;"> 模型正则化（一个标准化的解决方案）</p>

### <span id="Regularization">模型正则化</span>

#### 岭回归

回到这个例子：

<p style="align:center"><img src="./pngs/Polynomial-Regression-and-Model-Generalization_1.png" style="zoom:25%; "/></p>

这是多项式回归算法过拟合后产生的模型。模型非常的陡峭，实际上如果观察模型的数据值，可以发现，多项式的每个项前面的系数$(\theta_0 + \theta_1 x_1 + \theta_1 x_1 \cdots)$，那个$\theta$值，有些系数会非常大。

模型正则化，就是希望可以限制这个系数的大小，在这里用[一段代码](../notebooks/chp6-Polynomial-Regression-and-Model-Generalization/07-Model-Regularization.ipynb)来整这个$\theta$值，以及展示正则化。

模型正则化怎么解决这个问题呢？回到线性回归的目标：

使得
$$
\sum_{i=1}^m (y^{(i)} - \theta_0 - \theta_1X_1^{(i)} - \theta_2X_2^{(i)}-\cdots\ \theta_nX_n^{(i)})
$$
尽可能的小。换句话说，就是，使得目标函数：
$$
J(\theta) = MSE(y, \hat{y}; \theta)
$$
尽可能的小，让真实值与预测值的MSE尽可能的小，去调整$\theta$。

模型正则化旨在限制参数$\theta$的值。那么对于上述目标函数来说，加入模型正则化后，目标就变成了,使得
$$
J(\theta) = MSE(y, \hat{y}; \theta) + \alpha\frac{1}{2}\sum_{i=1}^n\theta_i^2
$$
尽可能小的。

前半部分的MSE不变，加入的部分是，所有的参数的平方和的一半乘以某个常数$\alpha$。这样做之后，要使得损失函数$J$最小，就不紧要顾及MSE这一项，还需要考虑到后面这一项，而后面这一项式所有的系数的平方和，使得这一项最小就意味着需要使得所有的系数都尽可能小。

当然需要注意一些细节：

* 后半部分的求和中，是1到n，**不考虑$\theta_0$这一项**，因为$\theta_0$不是任何一个X的系数，他是个截距，只决定整个曲线的高低。不决定每一部分的陡峭和缓和。

* 二分之一加不加都可以，因为使用梯度下降法优化这个目标函数$J$的时候需要求导，后面的$\theta_i$头顶上有个平方，微分以下这个平方就下来了，如果有个二分之一，就可以吃掉这个平方引入的2。不要也行

* 这个$\alpha$是一个新的超参数，因为新的目标中包含了似的所有的系数的平方和尽可能的小，这个小的程度占整个优化损失函数程度的多少。

  * 等于0的时候就意味着没有模型正则化
  * 如果是正无穷的话，前面的MSE的比重就很小，主要的优化任务就是**使得所有的$\theta_i$最小**，极端情况下所有的$\theta_i$都得是0，才能最小
  * 对于不同的数据，需要考虑是重点优化MSE，还是重点优化屁股后面的那个系数的平方和，需要对$\alpha$进行调惨

* $\alpha$是个常数，乘以屁屁后面的二分之一，也是个常数，不加入二分之一也是可以的，因为这一部分和$\alpha$本身是融合在一起的。 

模型正则化的方式有很多，像是上面这张加入了屁股后面这一项的方式，通常叫做**岭回归 Ridge Regression**，山岭的意思。

[岭回归代码](../notebooks/chp6-Polynomial-Regression-and-Model-Generalization/08-Ridge-Regreesion.ipynb)

#### LASSO Regularization

LASSO Regression 的全称是这个：Least Absolute Shrinkage and Selection Operator Regression。

注意到其中的Absolute了吗？没错，绝对值。

当然还有另外一个关键词，就是**Selection Operator**，选择运算，一个回归方法和选择运算有什么关系呢？后文将会阐述，这里先提示一下，记住这个LASSO回归具有一定的Selection功能。

在岭回归中，我们给目标函数添加了
$$
\alpha \frac{1}{2}\sum_{i=1}^n\theta_i^2
$$
这样一个带有平方的项，来控制系数的大小，从而对模型进行正则化，LASSO中，添加的是含有绝对值的项目：
$$
\alpha\sum_{i=1}^n |\theta_i|
$$
* 其中，alpha依然控制优化过程中，系数这一项的优化占比

对于线性回归算法的目标函数来说：

* 原始版本：
  $$
  J(\theta) = MSE(y, \hat{y}; \theta)
  $$
* 岭回归：
  $$
  J(\theta) = MSE(y, \hat{y}; \theta) + \alpha \frac{1}{2}\sum_{i=1}^n\theta_i^2
  $$
* LASSO：
  $$
  J(\theta) = MSE(y, \hat{y}; \theta) + \alpha\sum_{i=1}^n |\theta_i|
  $$
  

具体与岭回归的不同，[参见代码](../notebooks/chp6-Polynomial-Regression-and-Model-Generalization/09-LASSO.ipynb)

<table><tr>
 <td><img src="./pngs/Polynomial-Regression-and-Model-Generalization_7.png" style="zoom:70%; " border=0/></td>
<td><img src="./pngs/Polynomial-Regression-and-Model-Generalization_8.png" style="zoom:70%; " border=0/></td>
</tr></table>

比较Ridge与LASSO。

* 使用Ridge很难让模型变成一根直线，总是能弯曲一些，意味着有很多X(aka 特征），前面依然是有系数的
* LAASO获得曲线的弯曲程度更低，更倾向于变成一根直线，意味着有很多特征前面的系数变成0

LASSO趋向于使得一部分特征的系数值变为0，所以可以作为特征选择用。所以为什么有一个Selection Operator。

经过LASSO以后，一部分特征前面的系数变成0，意味着LASSO过程中，这一部分特征被认为是没有用的。反之就是有用的。

LASSO与PCA是有区别的，PCA是特征转换，降维后的每一个新的维度，其实都包含了原有特征的信息（只是占比不同），但是，降维后的新的维度，丢失了原有维度的语义信息。，LASSO是一个特征选择，通LASSO，**一些特征没有了**。

比如一个包含温度和大小的特征，降为一维，LASSO以后，就留下了温度或者大小某一个维度。但是PCA以后，留下的这个维度，既不是温度，也不是大小，是结合这二者的维度。



特征选择是怎么进行的呢？

先看岭回归中添加的一项：
$$
\alpha \frac{1}{2}\sum_{i=1}^n\theta_i^2
$$
当alpha趋于无穷时（只看后面的这一项），theta是逐渐变为0的，但是怎么变为0的，是有一个过程的，按照梯度下降法的角度来看，这个被添加的项的梯度是：
$$
\nabla = \alpha \left \{ 
\begin{matrix}
	\theta_1 \\
	\theta_2 \\
	\theta_3 \\
	\cdots \\
	\theta_n \\
\end{matrix}
\right\}
$$
每一个式子对每一个$\theta_i$求导，平方求导出来的2与前一项的二分之一抵消，系数就剩下alpha。但是要注意，这里面的每一个$\theta_i$，都是有值的。

所以假设有一个随机的初始点，按照这样的每一个维度都有值的梯度进行下降的话，会产生一个连续的轨迹（因为梯地下降的过程中，每一步$\theta$都是有值的）：

<p style="align:center"><img src="./pngs/Polynomial-Regression-and-Model-Generalization_9.png" style="zoom:25%; "/></p>

对于LASSO来说，若使得alpha趋近于无穷时（aka后一项的优化权最大的时候），但是这一项有个绝对值，不是处处可导的，但是可以使用sign函数来刻画：
$$
\nabla = \alpha
\left \{
    \begin{matrix}
		sign(\theta_0) \\ 
		sign(\theta_1) \\ 
		\cdots \\ 
		sign(\theta_n)
    \end{matrix}
\right \}
$$
其中，sign函数为：
$$
sign(x) = \left \{
  \begin{align}
    1, x > 0 \\
    0, x = 0 \\
    -1, x < 0
  \end{align}
\right.
$$
不考虑前面的MSE的时候，它的梯度就是alpha乘以一个向量，这个向量就只有-1，0，1这三种，梯度下降的时候，从初始点出发，会绘制出这样的路径：

<p style="align:center"><img src="./pngs/Polynomial-Regression-and-Model-Generalization_10.png" style="zoom:50%; "/></p>

不能绘制出一条曲线。在这个过程中，他就会走到某些轴的0点，使得最终的结果中包含了一堆0。

这也解释了为什么岭回归叫做岭回归，因为它的过程类似于爬山，不断的找坡缓的地方一点一点的下来，翻山越岭。

LASSO可以作为特征选择，但是可能出毛病：把一些有用的特征也变成0，所以从计算的准确度来说，LASSO不如岭回归。

另外一方面，如果系数特别大，比如degree特别大的时候，LASSO可以迅速的使系数变小。



#### LASSO，岭回归的总结，以及弹性网：

 首先先比较一下 Ridge和LASSO：

* 都在目标函数后面添加了表达式
* 但是表达式对$\theta$的约束不同

实际上，这个和MSE，MAE，欧拉距离，曼哈顿距离有异曲同工之妙：

|               Ridge                |                     MSE                      |                  欧拉距离                   |
| :--------------------------------: | :------------------------------------------: | :-----------------------------------------: |
| $\frac{1}{2}\sum_{i=1}^n \theta^2$ | $\frac{1}{2}\sum_{i=1}^n (y_i - \hat{y})^2 $ | $\sqrt{\sum{i=1}^n(x_i^{(1)}-x_i^{(2)})^2}$ |
|               LASSO                |                     MAE                      |                 曼哈顿距离                  |
| $\frac{1}{2}\sum_{i=1}^n |\theta|$ |  $\frac{1}{2}\sum_{i=1}^n |y_i - \hat{y}| $  |  $\sqrt{\sum{i=1}^n|x_i^{(1)}-x_i^{(2)}|}$  |

在机器学习领域，人们创造了许多名次来描述不同的东西在不同领域的应用，比如：

* Ridge/LASSO用于描述模型正则化
* MSE/MAE来描述模型的误差
* 欧拉距离，曼哈顿距离来描述两点之间的距离

这些东西的作用不同，描述不同，用法不同，但是他们背后的数学思想却是一样的，表示出来的数学含义也是很类似的。

这里想要描述的其实是，机器学习领域有很多不同的概念，用于描述各种东西，但是他们背后的数学含义是很紧密的，需要将他们有机的结合起来。

在[kNN相关的章节](./02-kNN.md)中，最后提到了一个叫做明可夫斯基距离的东西：

$$
\left(
\sum_{i=1}^n|X_i^{(a)} - X_i^{(b)}|^p
\right)^{\frac{1}{p}}\tag{Minkowski Distance}
$$


如果将这种形式进行泛化，提炼成这样：
$$
||X||_p = \left(
	\sum_{i=1}^n |X_i|^p
\right)^{\frac{1}{p}} \tag{Lp 范数}
$$
的形式。意思是，对于任何一个向量$X$，我们都可以求等式右边的一个值，对这个向量的第$i$个维度的值的p次方求和，然后对求和后的数值开$p$次方。

数学书叫做L的p范数，比如p=1时（其实就是求0点到这个向量的曼哈顿距离），就是L1范数，p=2（其实就是求0点到这个点的欧拉距离）时就是L2范数。

结合Lp范数，回头看：
* 岭回归的那个表达式，是一个平方项，这个叫做L2正则项（虽然没有开方）
* LASSO所添加的就是L1正则项

实际上以此类推，根据Lp范数往下写，可以用 正则项，但是在模型正则化的过程中，很少有使用p大于2的正则项。但是在数学上是存在这些正则项的。

最后，还有一个L0正则项，其定义是依然是给损失函数加入一个东西：
$$
J(\theta) = MSE(y,\hat y; \theta) + 
\color{#FF0000}{
min \left\{ 
		NumberOfNonZero\theta
\right\}
}
\tag{L0正则化}
$$
其目的在于，让theta 的个数尽量小，使得theta越小越好，后面一项描述的是非0theta的个数。

实际上很少使用L0正则，实际用L1取代，因为L0正则的优化是一个NP难的问题，不能使用梯度下降法/求解数学公式的方法来优化，屁股后面那一项是一个离散项，是一个离散最优化的问题，可能需要穷居所有让各种theta的组合为0的可能性，来依次计算损失函数，然后来决定让哪个theta为0，哪些不为0。

最后，在本文最后介绍最后一个概念：**弹性网 Elastic Net**

弹性网结合了L1，L2正则项两种正则化方式，对损失函数进行修饰：
$$
J(\theta) = MSE(y,\hat(y); \theta) + r \alpha\sum_{i=1}^n|\theta_i| + \frac{1-r}{2} \alpha\sum_{i=1}^n \theta_i^2
\tag{Elastic Net}
$$

* 实际上是在MES损失的屁股后面同时加入了L1和L2两个正则项
* 并且加入超参数r
* 超参数r用于控制L1/L2的优化比例 
* 同时结合了L1和L2的优点

在模型正则化的时候，应该首先尝试岭回归，因为相对精准（如果计算能力承受得住，系数太多的话算起来要炸），其次使用LASSO，LASSO急于将某些theta化为0，可能会产生一些错误，偏差（bias）可能会变大。

弹性网结合了两种方式，这种结合两个方法的优势的做法不是很罕见，比如之前的mini-Batch梯度下降法。



本章从多项式回归出发，引出了过拟合，欠拟合问题，并且引入了对应的解决方法。

