# 集成学习和随机森林

目录：

* [什么是集成学习](#Ensembel-Learning)
* [Soft Voting Classifier](#Soft-Voting)
* [Bagging and Pasting](#Bagging-Pasting)
* [oob(Out of Bag)和关于Bagging的更多讨论](#oob)
* [随机森林和Extra-Tree](#Random-Forest)
* [Ada Boosting和 Gradient Boosting](#Boosting)
* [Stacking](#Stacking)
* [XGBoost](#XGBoost)

#### <span id="Ensembel-Learning">什么是集成学习</span>

其实本质上，有点像是投票。比如，现在某人想买一个新的电脑，预算之内他看好了A与B两款，此时他就面临着一个分类，或者说2分类类似的决策问题。为了做出选择：

1. 他自己尝试收集A与B的相关参数做对比。

2. 问周围的朋友，有没有购买过A与B中的任意一款，并询问使用体验。
3. 找若干对电脑比较懂行的朋友，问其对A与B的看法，询问哪个比较值得购买。

实际上，2与3，就是一个集成学习的例子：不是只考虑某一个意见，而是将多个人的意见进行整理，最后做出决定。

换一个例子，在医院里，进行病情确诊的时候，对于一个复杂的病患，进行了一系列的检查，然后不同的专家，医生进行会诊，多个人的意见进行统一，最终做出决定，这也是一个很好的集成学习的例子。

机器学习领域有众多算法：

* kNN
* SVM
* 逻辑回归
* 神经网络
* 贝叶斯

它们可以解决分类问题，或者是回归问题，或者二者都可以，面对一个问题的时候，使用多个机器学习的算法进行决策，每个算法都各有优劣，并且决策方式不同，多个算法作出的多个决策也是不同的，将这些结果进行投票统计，最终以多的一方做为决策结果输出。

原理并不难，自己手写一下也很简单，但是sklearn中，提供了一个叫做Voting Classifier的类，可以方便地构建集成学习。[参考代码](../notebooks/chp11-Ensemble-Learning-and-Random-Forest/01-What-is-Ensemble-Learning.ipynb)

这种采用少数服从多数决策方法的投票方式，有叫做Hard Voting，相对于Hard，也有Soft Voting。

#### <span id="Soft-Voting">Soft Voting Classifier</span>

少数服从多数可能在很多时候并不是一个合理的方法，比如在社会学中有个词叫做[民主暴政](https://zh.wikipedia.org/wiki/%E5%A4%9A%E6%95%B8%E4%BA%BA%E6%9A%B4%E6%94%BF)，苏格拉底是怎么死的？

更合理的投票方式，应该有权值。专业人士的票应该更加权威，比如经济政策的指定中，经济领域的专家的意见更应该被考虑。

面对一个2分类问题，使用5个模型：

| 模型 | 模型判断是A类的概率（百分比） | 模型判断是B类的概率（百分比） |
| :--: | :---------------------------: | :---------------------------: |
|  1   |              99               |               1               |
|  2   |              49               |              51               |
|  3   |              40               |              60               |
|  4   |              90               |              10               |
|  5   |              30               |              70               |

显然：

* 模型1与4将数据分为A类

* 模型2，3，5将数据分为B类

此时，采用Hard Voting 的方式来决定分类的话，这个数据将会被分类为B。

但是，看概率：

* 1，4两个模型非常确定的告诉我们是A，非常确信
* 2，3，5两个模型的判断虽然是B，但是他们自己都不太确定是不是B，或者说给出的概率不是很高。

Soft Voting算法中，每个模型都给出了分为A，B两个类的概率，综合来说：

1. 分为A的概率为：
   $$
   P(A) = \frac{(0.99 + 0.49 + 0.4 + 0.9 + 0.3)}{5} = 0.616
   $$
   

2. 分为B的概率为：
   $$
   P(B) = \frac{(0.1 + 0.51 + 0.6 + 0.1 + 0.7)}{5} = 0.384
   $$

通过这样的计算，最终结果为A。

考虑了：

* 两边的票数
* 投票者给出的概率

如果使用Soft Voting：

* 要求集成的每一个模型都可以估计概率
  * 逻辑回归算法，本身就基于概率模型
  * kNN算法中有个投票，两个类的投票的占比也是个概率
  * 决策树，占比最大的数据点的数量除以某个叶子结点中所有数据点的数量，也可以得到一个概率
  * SVM算法，其算法本身不考虑概率，但是消耗更多的资源也是可以计算概率的。[sklearn文档](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)。

[参考代码](../notebooks/chp11-Ensemble-Learning-and-Random-Forest/02-Soft-Voting-Classifier.ipynb)

#### <span id="Bagging-Pasting">Bagging and Pasting</span>

从投票的角度看，仍然不够多。如果希望最终的结果更加可信，就需要更多的投票者，甚至于需要成百上千的投票者。参考概率论中的[大数定律](https://zh.wikipedia.org/wiki/%E5%A4%A7%E6%95%B8%E6%B3%95%E5%89%87)。这就意味着需要更多的字模型，集成更多子模型的意见，并且子模型之间不能一致，子模型之间要有差异性。

如何创建具有差异性的子模型呢？一个简单的想法就是：每个子模型只看样本数据的一部分。

一共有500个数据样本，每个子模型只看100个样本数据。这样做会大大降低子模型的准确度，但是集成学习的过程中，每个子模型不需要太高的准确率，这就是集成学习神器的地方。

如果每个子模型很自由51%的准确率（这已经蛮低了，扔个硬币都五五开）

* 只有一个子模型的时候，整体准确率为51%
* 有三个子模型的时候，整体准确率为$0.51^3 + C_3^2 \cdot 0.51^2 \cdot 0.49$，这个表达式意思是，三个子模型都说对了的概率+从三个子模型中挑出两个$C_3^2$说对了的概率0.51的平方，再乘以另一个子模型说错了的概率0.49。整体准确率为51.5%
* 有500个子模型，整体准确率为：$\sum_{i=251}^{500} C_{500}^i 0.51^i\cdot 0.49^{500-i}$，意思是只要有多与一半的子模型（251个）说对了的概率，去乘以每一次有$i$个子模型说对了的概率。整体准确率就是65.6%

假设每个子模型的准确率是60%，比扔硬币好一些，500个子模型的整体准确率就是：
$$
\sum_{i=251}^{500} C_{500}^i 0.6^i\cdot 0.4^{500-i}
$$
整体准确率达到了惊人的99.999%。

这只是一个极端的例子，主要想表达每一个子模型不需要太高的准确率，集成以后的准确率也可以很高的。

子模型准确率有高有低，有时候准确率低于50%，犯错误的概率高于正确的概率。

正因为集成学习有这样的性质，在机器学习算法竞赛中，经常被使用。

那么如何让每个子模型只看样本数据的一部分呢？

有两个方式，取样后：

* 放回取样，一个子模型看完100个样本，放回去，下一个子模型再来看，子模型看到的样本之间可能有重复。这就叫做Bagging
* 不放回取样，500个样本，一个看完100个，拿走，剩下400个给下一个子模型看，以此类推，子模型看到的样本之间没有重复。这就叫做Pasting

通常来说，Bagging更常用。500个样本使用Pasting，每个只能看100个数据的话，只能创建五个子模型，反而bagging可以创建成百上千个子模型。

另外，Bagging不那么依赖于随机。Pasting非常依赖于随机分割，分割方式将强烈的影响集成学习。

当然，统计学中，放回取样叫做bootstrap。

[参考代码](../notebooks/chp11-Ensemble-Learning-and-Random-Forest/03-Bagging-and-Pasting.ipynb)

参考代码中使用的子模型都是决策树，原因很简单：决策树这种非参数学习的模型本身就很容易产生出差异很大的模型。集成学习需要差异大的子模型。

#### <span id="oob">oob(Out of Bag)和关于Bagging的更多讨论</span>

OOB， Out of bag。放回取样导致一部分样本很有可能没有取到，平均大约有37%的样本没有取到。从来没有被取到的样本，就是被称为OOB。

如果是这样，机器学习的过程中就不需要分割测试/训练数据集了，转而使用这部分没有被取样到的样本做测试/验证。

在sklearn中就有一个叫做oob_score_的属性来表示，使用没有被取样过的样本作为测试数据集计算后的模型的准确度。

另外，对于Bagging的思路来说，是极易并行化处理的。

产生差异的方式有很多，比如前文中提到的针对取样的方式，还有一种方式是针对特征的（Random Subspace）。特征量比较多时候可以使用这种方式，比如图像识别，子模型只看图片的一部分。

既针对样本，又针对特征进行随机采样，这种方式叫做Random Patches，Patche就是补丁的意思，很好理解。

数据矩阵，每一行都是个样本，每一列都是样本的某一个维度的特征，不仅随机的看行（随机看样本），还随机看样本的特征（随机看列），这就在数据矩阵中形成了随机的小方块，这些小方块可以想象成一个个小补丁。这个方式在sklearn中使用boostrap_feature和max_feature参数来控制。

这种随机方式还有个名字，叫做随机森林。

[参考代码](../notebooks/chp11-Ensemble-Learning-and-Random-Forest/04-OOB-and-More-about-Bagging-Classifier.ipynb)

#### <span id="Random-Forest">随机森林和Extra-Tree</span>

[参考代码](../notebooks/chp11-Ensemble-Learning-and-Random-Forest/05-Random-Forest-and-Extra-Trees.ipynb)

#### <span id="Boosting">Ada Boosting和 Gradient Boosting</span>

[参考代码](../notebooks/chp11-Ensemble-Learning-and-Random-Forest/06-AdaBoost-and-Gradient-Boosting.ipynb)

#### <span id="Stacking">Stacking</span>

#### <span id="XGBoost">XGBoost</span>